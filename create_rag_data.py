import sys
import json
import argparse
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional
import warnings

from config import *

warnings.filterwarnings('ignore')

# ============================================================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================================================
DEFAULT_CONFIG = {
    "input_dir": INPUT_DIR,
    "output_dir": OUTPUT_DIR,
    "chunk_size": CHUNKS_SIZE,
    "chunk_overlap": CHUNK_OVERLAP,
    "embedding_model": EMBEDDING_MODEL,
    "faiss_index_type": "flat",  # flat, ivf, hnsw
    "min_chunk_length": 50,  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
    "max_chunk_length": 2000,  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞
    "device": "cpu",  # cpu, mps, cuda
}

# ============================================================================
# –£–¢–ò–õ–ò–¢–´ –î–õ–Ø –†–ê–ë–û–¢–´ –° –§–ê–ô–õ–ê–ú–ò
# ============================================================================

def find_text_files(directory: str) -> List[str]:
    """–ù–∞–π—Ç–∏ –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    extensions = ['.md', '.markdown', '.txt', '.MD', '.Markdown', '.TXT']
    files = []
    
    for ext in extensions:
        files.extend(list(Path(directory).rglob(f"*{ext}")))
    
    return sorted([str(f) for f in files])

def read_file_with_encodings(filepath: str) -> str:
    """–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫–∏"""
    encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1251', 'cp866', 'iso-8859-1']
    
    for encoding in encodings:
        try:
            with open(filepath, 'r', encoding=encoding) as f:
                return f.read()
        except (UnicodeDecodeError, UnicodeError):
            continue
    
    # –ï—Å–ª–∏ –Ω–∏ –æ–¥–Ω–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∞ –Ω–µ –ø–æ–¥–æ—à–ª–∞, –ø—Ä–æ–±—É–µ–º —Å –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—à–∏–±–æ–∫
    try:
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()
    except Exception as e:
        print(f"    ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {filepath}: {e}")
        return ""

def clean_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return ""
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
    lines = []
    for line in text.split('\n'):
        line = line.rstrip()
        if line or (lines and lines[-1]):
            lines.append(line)
    
    return '\n'.join(lines)

# ============================================================================
# –°–û–ó–î–ê–ù–ò–ï –ß–ê–ù–ö–û–í (–ü–†–ê–í–ò–õ–¨–ù–´–ô –§–û–†–ú–ê–¢)
# ============================================================================

class ChunkCreator:
    """–°–æ–∑–¥–∞—Ç–µ–ª—å —á–∞–Ω–∫–æ–≤ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
    
    def __init__(self, config: Dict):
        self.config = config
        
    def split_by_paragraphs(self, text: str) -> List[str]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∞–±–∑–∞—Ü—ã"""
        paragraphs = []
        current_paragraph = []
        
        for line in text.split('\n'):
            line = line.strip()
            if not line:
                if current_paragraph:
                    paragraphs.append(' '.join(current_paragraph))
                    current_paragraph = []
            else:
                current_paragraph.append(line)
        
        if current_paragraph:
            paragraphs.append(' '.join(current_paragraph))
        
        return paragraphs
    
    def split_long_paragraph(self, paragraph: str, chunk_size: int) -> List[str]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∞–±–∑–∞—Ü–∞ –Ω–∞ —á–∞–Ω–∫–∏"""
        words = paragraph.split()
        chunks = []
        current_chunk = []
        current_length = 0
        
        for word in words:
            word_length = len(word) + 1  # +1 –¥–ª—è –ø—Ä–æ–±–µ–ª–∞
            
            if current_length + word_length > chunk_size and current_chunk:
                chunks.append(' '.join(current_chunk))
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º overlap (–ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–ª–æ–≤)
                overlap_words = max(1, int(len(current_chunk) * 0.3))  # 30% –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
                current_chunk = current_chunk[-overlap_words:] if len(current_chunk) > overlap_words else current_chunk
                current_length = sum(len(w) + 1 for w in current_chunk) - 1
            
            current_chunk.append(word)
            current_length += word_length
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    def create_chunks_from_text(self, text: str, source: str) -> List[Dict]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        chunks = []
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã
        paragraphs = self.split_by_paragraphs(text)
        
        for para_idx, paragraph in enumerate(paragraphs):
            para_length = len(paragraph)
            
            # –ï—Å–ª–∏ –∞–±–∑–∞—Ü —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if para_length < self.config["min_chunk_length"]:
                continue
            
            # –ï—Å–ª–∏ –∞–±–∑–∞—Ü —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ
            if para_length > self.config["chunk_size"]:
                para_chunks = self.split_long_paragraph(paragraph, self.config["chunk_size"])
                for chunk_idx, chunk_text in enumerate(para_chunks):
                    if len(chunk_text) < self.config["min_chunk_length"]:
                        continue
                    
                    chunks.append({
                        "text": chunk_text,
                        "source": source,
                        "paragraph": para_idx,
                        "chunk_in_paragraph": chunk_idx,
                        "char_count": len(chunk_text),
                        "word_count": len(chunk_text.split())
                    })
            else:
                # –ê–±–∑–∞—Ü –ø–æ–¥—Ö–æ–¥–∏—Ç –∫–∞–∫ —Ü–µ–ª—ã–π —á–∞–Ω–∫
                chunks.append({
                    "text": paragraph,
                    "source": source,
                    "paragraph": para_idx,
                    "chunk_in_paragraph": 0,
                    "char_count": para_length,
                    "word_count": len(paragraph.split())
                })
        
        return chunks
    
    def process_files(self) -> List[Dict]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤"""
        print("üîß –°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤...")
        
        input_dir = Path(self.config["input_dir"])
        if not input_dir.exists():
            print(f"‚ùå –í—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {input_dir}")
            return []
        
        # –ù–∞—Ö–æ–¥–∏–º —Ñ–∞–π–ª—ã
        files = find_text_files(str(input_dir))
        if not files:
            print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –≤ {input_dir}")
            print("   –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è: .md, .markdown, .txt")
            return []
        
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤")
        
        all_chunks = []
        for file_idx, filepath in enumerate(files, 1):
            filename = Path(filepath).name
            print(f"  [{file_idx}/{len(files)}] üìÑ {filename}")
            
            # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
            content = read_file_with_encodings(filepath)
            if not content.strip():
                print(f"     ‚ö†Ô∏è  –§–∞–π–ª –ø—É—Å—Ç")
                continue
            
            # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            cleaned_content = clean_text(content)
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤
            file_chunks = self.create_chunks_from_text(cleaned_content, filepath)
            
            print(f"     ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(file_chunks)} —á–∞–Ω–∫–æ–≤")
            all_chunks.extend(file_chunks)
        
        # –î–æ–±–∞–≤–ª—è–µ–º ID –∫ —á–∞–Ω–∫–∞–º
        for idx, chunk in enumerate(all_chunks):
            chunk["id"] = idx
        
        print(f"\nüéâ –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤")
        return all_chunks
    
    def save_metadata(self, chunks: List[Dict], output_path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ –ü–†–ê–í–ò–õ–¨–ù–û–ú —Ñ–æ—Ä–º–∞—Ç–µ"""
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ metadata.json...")
        
        # –ü–†–ê–í–ò–õ–¨–ù–´–ô –§–û–†–ú–ê–¢: –ø—Ä–æ—Å—Ç–æ —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
        metadata = chunks  # –£–∂–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ metadata.json —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {len(metadata)} –∑–∞–ø–∏—Å–µ–π")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—Ç–¥–µ–ª—å–Ω–æ
        stats = self.calculate_statistics(chunks)
        stats_path = Path(output_path).parent / "statistics.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"üìä statistics.json —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–≤—å—é
        self.create_preview(chunks, Path(output_path).parent / "chunks_preview.txt")
    
    def calculate_statistics(self, chunks: List[Dict]) -> Dict:
        """–†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —á–∞–Ω–∫–∞–º"""
        if not chunks:
            return {}
        
        char_counts = [chunk["char_count"] for chunk in chunks]
        word_counts = [chunk["word_count"] for chunk in chunks]
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ñ–∞–π–ª–∞–º
        files = {}
        for chunk in chunks:
            source = chunk["source"]
            if source not in files:
                files[source] = {
                    "filename": Path(source).name,
                    "chunks": 0,
                    "total_chars": 0,
                    "total_words": 0
                }
            files[source]["chunks"] += 1
            files[source]["total_chars"] += chunk["char_count"]
            files[source]["total_words"] += chunk["word_count"]
        
        return {
            "total_chunks": len(chunks),
            "total_files": len(files),
            "avg_chars_per_chunk": sum(char_counts) / len(char_counts),
            "min_chars_per_chunk": min(char_counts),
            "max_chars_per_chunk": max(char_counts),
            "avg_words_per_chunk": sum(word_counts) / len(word_counts),
            "min_words_per_chunk": min(word_counts),
            "max_words_per_chunk": max(word_counts),
            "files": files
        }
    
    def create_preview(self, chunks: List[Dict], preview_path: str):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–≤—å—é —á–∞–Ω–∫–æ–≤"""
        print(f"üëÄ –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–≤—å—é...")
        
        with open(preview_path, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("–ü–†–ï–í–¨–Æ –°–û–ó–î–ê–ù–ù–´–• –ß–ê–ù–ö–û–í\n")
            f.write("=" * 80 + "\n\n")
            
            for i, chunk in enumerate(chunks[:10]):  # –ü–µ—Ä–≤—ã–µ 10 —á–∞–Ω–∫–æ–≤
                f.write(f"–ß–ê–ù–ö {i+1} (ID: {chunk['id']})\n")
                f.write(f"–§–∞–π–ª: {Path(chunk['source']).name}\n")
                f.write(f"–ê–±–∑–∞—Ü: {chunk['paragraph']}, –ß–∞–Ω–∫: {chunk['chunk_in_paragraph']}\n")
                f.write(f"–°–∏–º–≤–æ–ª–æ–≤: {chunk['char_count']}, –°–ª–æ–≤: {chunk['word_count']}\n")
                f.write("-" * 40 + "\n")
                f.write(chunk['text'][:300] + ("..." if len(chunk['text']) > 300 else "") + "\n")
                f.write("=" * 80 + "\n\n")

# ============================================================================
# –°–û–ó–î–ê–ù–ò–ï –≠–ú–ë–ï–î–î–ò–ù–ì–û–í
# ============================================================================

class EmbeddingCreator:
    """–°–æ–∑–¥–∞—Ç–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    
    def __init__(self, config: Dict):
        self.config = config
        
    def create_embeddings(self, chunks: List[Dict]) -> Optional[np.ndarray]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —á–∞–Ω–∫–æ–≤"""
        print("\nüß† –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        
        try:
            from sentence_transformers import SentenceTransformer
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            print(f"  –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {self.config['embedding_model']}")
            model = SentenceTransformer(
                self.config['embedding_model'],
                device=self.config['device']
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã
            texts = [chunk["text"] for chunk in chunks]
            print(f"  –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤...")
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            embeddings = model.encode(
                texts,
                show_progress_bar=True,
                normalize_embeddings=True  # –î–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏
            )
            
            print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã: {embeddings.shape}")
            return embeddings
            
        except ImportError:
            print("‚ùå sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers")
            return None
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            return None
    
    def save_embeddings(self, embeddings: np.ndarray, output_path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        
        np.save(output_path, embeddings)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö
        info = {
            "embedding_model": self.config["embedding_model"],
            "embedding_dim": embeddings.shape[1],
            "num_embeddings": embeddings.shape[0],
            "normalized": True
        }
        
        info_path = Path(output_path).parent / "embeddings_info.json"
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(info, f, indent=2)
        
        print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
        print(f"   –†–∞–∑–º–µ—Ä: {embeddings.shape[0]} x {embeddings.shape[1]}")

# ============================================================================
# –°–û–ó–î–ê–ù–ò–ï FAISS –ò–ù–î–ï–ö–°–ê
# ============================================================================

class FaissIndexCreator:
    """–°–æ–∑–¥–∞—Ç–µ–ª—å FAISS –∏–Ω–¥–µ–∫—Å–∞"""
    
    def __init__(self, config: Dict):
        self.config = config
        
    def create_index(self, embeddings: np.ndarray) -> Any:
        """–°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞"""
        print("\nüîç –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
        
        try:
            import faiss
            
            dimension = embeddings.shape[1]
            print(f"  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {dimension}")
            print(f"  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {embeddings.shape[0]}")
            
            # –í—ã–±–∏—Ä–∞–µ–º —Ç–∏–ø –∏–Ω–¥–µ–∫—Å–∞
            if self.config["faiss_index_type"] == "flat":
                # –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫
                index = faiss.IndexFlatIP(dimension)  # Inner Product –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π
                print("  –¢–∏–ø: FlatIP (—Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫)")
                
            elif self.config["faiss_index_type"] == "ivf":
                # –ü—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ (–±—ã—Å—Ç—Ä–µ–µ)
                nlist = min(100, embeddings.shape[0] // 39)
                quantizer = faiss.IndexFlatIP(dimension)
                index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)
                
                # –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ
                print(f"  –û–±—É—á–µ–Ω–∏–µ IVF –∏–Ω–¥–µ–∫—Å–∞ (nlist={nlist})...")
                index.train(embeddings)
                index.nprobe = 10
                print("  –¢–∏–ø: IVF (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫)")
                
            elif self.config["faiss_index_type"] == "hnsw":
                # HNSW (–∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–π –º–∞–ª—ã–π –º–∏—Ä)
                index = faiss.IndexHNSWFlat(dimension, 32)
                index.hnsw.efConstruction = 200
                index.hnsw.efSearch = 128
                print("  –¢–∏–ø: HNSW (–±—ã—Å—Ç—Ä—ã–π –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫)")
                
            else:
                print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∏–Ω–¥–µ–∫—Å–∞: {self.config['faiss_index_type']}")
                return None
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä—ã –≤ –∏–Ω–¥–µ–∫—Å
            print("  –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤...")
            index.add(embeddings)
            
            print(f"‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: {index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")
            return index
            
        except ImportError:
            print("‚ùå faiss –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install faiss-cpu (–∏–ª–∏ faiss-gpu –¥–ª—è CUDA)")
            return None
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")
            return None
    
    def save_index(self, index: Any, output_path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞"""
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞...")
        
        try:
            import faiss
            
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            
            faiss.write_index(index, output_path)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏–Ω–¥–µ–∫—Å–µ
            info = {
                "index_type": self.config["faiss_index_type"],
                "num_vectors": index.ntotal,
                "dimension": index.d,
                "faiss_version": faiss.__version__
            }
            
            info_path = Path(output_path).parent / "faiss_index_info.json"
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(info, f, indent=2)
            
            size_mb = Path(output_path).stat().st_size / 1024 / 1024
            print(f"‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
            print(f"   –†–∞–∑–º–µ—Ä: {size_mb:.2f} MB")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {e}")
    
    def test_index(self, index: Any, embeddings: np.ndarray, num_tests: int = 3):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞"""
        print("\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
        
        try:
            import faiss
            
            for i in range(min(num_tests, embeddings.shape[0])):
                # –ë–µ—Ä–µ–º i-–π –≤–µ–∫—Ç–æ—Ä –∫–∞–∫ —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
                query = embeddings[i:i+1]
                
                # –ò—â–µ–º 3 –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–∞
                distances, indices = index.search(query, k=3)
                
                print(f"  –¢–µ—Å—Ç {i+1}:")
                print(f"    –ù–∞–π–¥–µ–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã: {indices[0].tolist()}")
                print(f"    –†–∞—Å—Å—Ç–æ—è–Ω–∏—è: {distances[0].round(4).tolist()}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç - —Å–∞–º –≤–µ–∫—Ç–æ—Ä
                if indices[0][0] == i:
                    print(f"    ‚úÖ –í–µ–∫—Ç–æ—Ä –Ω–∞—à–µ–ª —Å–µ–±—è (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distances[0][0]:.4f})")
                else:
                    print(f"    ‚ö†Ô∏è  –í–µ–∫—Ç–æ—Ä –Ω–µ –Ω–∞—à–µ–ª —Å–µ–±—è –Ω–∞ –ø–µ—Ä–≤–æ–º –º–µ—Å—Ç–µ")
            
            print("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

# ============================================================================
# –û–°–ù–û–í–ù–û–ô –ü–ê–ô–ü–õ–ê–ô–ù
# ============================================================================

class RAGDataPipeline:
    """–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è RAG"""
    
    def __init__(self, config: Dict):
        self.config = config
        
        # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        self.output_dir = Path(config["output_dir"])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
        self.metadata_path = self.output_dir / "metadata.json"
        self.embeddings_path = self.output_dir / "embeddings.npy"
        self.faiss_index_path = self.output_dir / "faiss.index"
        
    def run(self, test_index: bool = True):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        print("üöÄ –ó–ê–ü–£–°–ö RAG DATA PIPELINE")
        print("=" * 60)
        
        # 1. –°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤
        creator = ChunkCreator(self.config)
        chunks = creator.process_files()
        
        if not chunks:
            print("‚ùå –ù–µ —Å–æ–∑–¥–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞")
            return False
        
        # 2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        creator.save_metadata(chunks, str(self.metadata_path))
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embedding_creator = EmbeddingCreator(self.config)
        embeddings = embedding_creator.create_embeddings(chunks)
        
        if embeddings is None:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏")
            return False
        
        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embedding_creator.save_embeddings(embeddings, str(self.embeddings_path))
        
        # 5. –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞
        index_creator = FaissIndexCreator(self.config)
        index = index_creator.create_index(embeddings)
        
        if index is None:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å")
            return False
        
        # 6. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        index_creator.save_index(index, str(self.faiss_index_path))
        
        # 7. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if test_index:
            index_creator.test_index(index, embeddings)
        
        print("\n" + "=" * 60)
        print("üéâ –ü–ê–ô–ü–õ–ê–ô–ù –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!")
        print("=" * 60)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        self.show_results()
        
        return True
    
    def show_results(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        print("\nüìÅ –°–û–ó–î–ê–ù–ù–´–ï –§–ê–ô–õ–´:")
        print(f"  üìÑ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {self.metadata_path}")
        print(f"  üß† –≠–º–±–µ–¥–¥–∏–Ω–≥–∏: {self.embeddings_path}")
        print(f"  üîç FAISS –∏–Ω–¥–µ–∫—Å: {self.faiss_index_path}")
        print(f"  üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {self.output_dir / 'statistics.json'}")
        print(f"  üëÄ –ü—Ä–µ–≤—å—é: {self.output_dir / 'chunks_preview.txt'}")
        print(f"  ‚ÑπÔ∏è  –ò–Ω—Ñ–æ –æ–± —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö: {self.output_dir / 'embeddings_info.json'}")
        print(f"  ‚ÑπÔ∏è  –ò–Ω—Ñ–æ –æ–± –∏–Ω–¥–µ–∫—Å–µ: {self.output_dir / 'faiss_index_info.json'}")
        
        print("\nü§ñ –î–õ–Ø –ó–ê–ü–£–°–ö–ê RAG –ë–û–¢–ê:")
        print(f"  python rag_bot_eng.py \\")
        print(f"    --model ./mistral-7b-instruct-v0.2.Q4_K_M.gguf \\")
        print(f"    --index {self.faiss_index_path} \\")
        print(f"    --metadata {self.metadata_path}")
        
        print("\nüí° –ë–´–°–¢–†–´–ô –¢–ï–°–¢:")
        print(f'  python rag_bot_eng.py \\')
        print(f'    --model ./mistral-7b-instruct-v0.2.Q4_K_M.gguf \\')
        print(f'    --index {self.faiss_index_path} \\')
        print(f'    --metadata {self.metadata_path} \\')
        print(f'    --question "What is machine learning?"')

# ============================================================================
# –ö–û–ú–ê–ù–î–ù–ê–Ø –°–¢–†–û–ö–ê
# ============================================================================

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    parser = argparse.ArgumentParser(
        description="–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s                         # –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
  %(prog)s --input ./my_docs      # –£–∫–∞–∑–∞—Ç—å —Å–≤–æ—é –ø–∞–ø–∫—É —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
  %(prog)s --chunk-size 800       # –£–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤
  %(prog)s --embed-model sentence-transformers/all-mpnet-base-v2  # –î—Ä—É–≥–∞—è –º–æ–¥–µ–ª—å
  %(prog)s --index-type ivf       # –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –±–æ–ª—å—à–∏—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π
  
–†–µ–∑—É–ª—å—Ç–∞—Ç:
  –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –ø–∞–ø–∫—É ./rag_data/
  metadata.json - –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ (–ø—Ä–æ—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫)
        """
    )
    
    parser.add_argument(
        "--input", "-i",
        default=INPUT_DIR,
        help="–í—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏"
    )
    
    parser.add_argument(
        "--output", "-o",
        default=OUTPUT_DIR,
        help="–í—ã—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤"
    )
    
    parser.add_argument(
        "--chunk-size", "-s",
        type=int,
        default=CHUNKS_SIZE,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö"
    )
    
    parser.add_argument(
        "--chunk-overlap", "-l",
        type=int,
        default=CHUNK_OVERLAP,
        help="–ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏"
    )
    
    parser.add_argument(
        "--embed-model", "-e",
        default=EMBEDDING_MODEL,
        help="–ú–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"
    )
    
    parser.add_argument(
        "--index-type", "-t",
        default="flat",
        choices=["flat", "ivf", "hnsw"],
        help="–¢–∏–ø FAISS –∏–Ω–¥–µ–∫—Å–∞"
    )
    
    parser.add_argument(
        "--device",
        default="cpu",
        choices=["cpu", "mps", "cuda"],
        help="–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (mps –¥–ª—è Mac M1/M2/M3)"
    )
    
    parser.add_argument(
        "--no-test",
        action="store_true",
        help="–ù–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è"
    )
    
    parser.add_argument(
        "--install-deps",
        action="store_true",
        help="–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –≤—ã–π—Ç–∏"
    )
    
    parser.add_argument(
        "--quick",
        action="store_true",
        help="–ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º (–º–µ–Ω—å—à–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)"
    )
    
    args = parser.parse_args()
    
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    if args.install_deps:
        print("üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")
        
        deps = [
            "sentence-transformers>=2.2.0",
            "faiss-cpu>=1.7.0",
            "numpy>=1.24.0",
        ]
        
        import subprocess
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install"] + deps)
            print("‚úÖ –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")
            print("\n–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞–π–ø–ª–∞–π–Ω:")
            print("  python rag_create_data.py")
        except subprocess.CalledProcessError as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏: {e}")
            print("\n–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –≤—Ä—É—á–Ω—É—é:")
            print("pip install sentence-transformers faiss-cpu numpy")
        return
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    config = DEFAULT_CONFIG.copy()
    config.update({
        "input_dir": args.input,
        "output_dir": args.output,
        "chunk_size": args.chunk_size,
        "chunk_overlap": args.chunk_overlap,
        "embedding_model": args.embed_model,
        "faiss_index_type": args.index_type,
        "device": args.device,
    })
    
    # –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º
    if args.quick:
        config["chunk_size"] = 300
        config["chunk_overlap"] = 30
        print("‚ö° –ë—ã—Å—Ç—Ä—ã–π —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
    
    try:
        # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
        pipeline = RAGDataPipeline(config)
        success = pipeline.run(test_index=not args.no_test)
        
        if not success:
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\n\n‚èπÔ∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(0)
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

# ============================================================================
# –ó–ê–ü–£–°–ö
# ============================================================================

if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ Python –≤–µ—Ä—Å–∏–∏
    if sys.version_info < (3, 7):
        print("‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è Python 3.7 –∏–ª–∏ –≤—ã—à–µ")
        sys.exit(1)
    
    # –ó–∞–ø—É—Å–∫
    main()
